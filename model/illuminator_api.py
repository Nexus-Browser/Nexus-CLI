"""
iLLuMinator API Client - Direct integration with iLLuMinator-4.7B model
Connects directly to the iLLuMinator-4.7B transformer model for local inference
Repository: https://github.com/Anipaleja/iLLuMinator-4.7B
"""

import os
import json
import time
import logging
import torch
from typing import Optional, Dict, List, Any
from pathlib import Path
import subprocess
import sys

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class iLLuMinatorAPI:
    """
    Direct API client for iLLuMinator-4.7B model from GitHub repository
    Provides local inference without external API dependencies
    """
    
    def __init__(self, model_path: str = "./model/nexus_model/", fast_mode: bool = False, use_gpu_acceleration: bool = True):
        """
        Initialize iLLuMinator-4.7B API with aggressive local optimizations.
        
        Args:
            model_path: Path to the local fine-tuned model
            fast_mode: Enable aggressive speed optimizations
            use_gpu_acceleration: Use GPU acceleration if available
        """
        self.model_path = model_path
        self.fast_mode = fast_mode
        self.use_gpu_acceleration = use_gpu_acceleration and self._check_gpu_availability()
        
        # Optimization settings
        self.max_length = 512 if fast_mode else 1024
        self.use_quantization = fast_mode
        
        # Device selection
        if self.use_gpu_acceleration:
            import torch
            self.device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
        else:
            import torch
            self.device = torch.device("cpu")
        
        
        # Load model with optimizations
        logger.info("ðŸš€ Loading local fine-tuned model with optimizations...")
        self._load_model_optimized()
    
    def _check_gpu_availability(self) -> bool:
        """Check if GPU acceleration is available."""
        try:
            import torch
            return torch.cuda.is_available() or torch.backends.mps.is_available()
        except ImportError:
            return False
        
        # Load model with optimizations
        logger.info("ðŸš€ Loading local fine-tuned model with optimizations...")
        self._load_model_optimized()
    
    def _check_available_apis(self) -> Dict[str, str]:
        """Check what cloud APIs are available for fast fallback."""
        available_apis = {}
        
        # Check environment variables for API keys
        env_files = ['.env', '.env.local']
        for env_file in env_files:
            if os.path.exists(env_file):
                with open(env_file, 'r') as f:
                    for line in f:
                        if '=' in line and not line.startswith('#'):
                            key, value = line.strip().split('=', 1)
                            if 'API_KEY' in key and value:
                                if 'COHERE' in key:
                                    available_apis['cohere'] = value
                                elif 'GEMINI' in key:
                                    available_apis['gemini'] = value
                                elif 'GROQ' in key:
                                    available_apis['groq'] = value
                                elif 'OPENAI' in key:
                                    available_apis['openai'] = value
        
        # Also check from COHERE_SUCCESS.md - we know Cohere is working
        if not available_apis and os.path.exists('COHERE_SUCCESS.md'):
            with open('COHERE_SUCCESS.md', 'r') as f:
                content = f.read()
                if 'wx6ib6ezwXClSarnEZU0FrK1eLJcVTqpCAHnfuTW' in content:
                    available_apis['cohere'] = 'wx6ib6ezwXClSarnEZU0FrK1eLJcVTqpCAHnfuTW'
        
        return available_apis

    def _use_cloud_api(self, prompt: str) -> str:
        """Use cloud API for fast responses when available."""
        if 'cohere' in self.fallback_apis:
            return self._call_cohere_api(prompt)
        elif 'gemini' in self.fallback_apis:
            return self._call_gemini_api(prompt)
        elif 'groq' in self.fallback_apis:
            return self._call_groq_api(prompt)
        else:
            return "Fast mode enabled but no cloud APIs available. Using local processing..."
    
    def _call_cohere_api(self, prompt: str) -> str:
        """Call Cohere API for fast responses using requests."""
        try:
            import requests
            
            headers = {
                'Authorization': f'Bearer {self.fallback_apis["cohere"]}',
                'Content-Type': 'application/json',
            }
            
            data = {
                'model': 'command-light',
                'prompt': prompt,
                'max_tokens': 300,
                'temperature': 0.7
            }
            
            response = requests.post(
                'https://api.cohere.ai/v1/generate',
                headers=headers,
                json=data,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['generations'][0]['text'].strip()
            else:
                logger.warning(f"Cohere API error: {response.status_code}")
                return self._fallback_to_local(prompt)
                
        except Exception as e:
            logger.warning(f"Cohere API failed: {e}")
            return self._fallback_to_local(prompt)
    
    def _call_gemini_api(self, prompt: str) -> str:
        """Call Gemini API for fast responses."""
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.fallback_apis['gemini'])
            model = genai.GenerativeModel('gemini-pro')
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.warning(f"Gemini API failed: {e}")
            return self._fallback_to_local(prompt)
    
    def _call_groq_api(self, prompt: str) -> str:
        """Call Groq API for fast responses."""
        try:
            from groq import Groq
            client = Groq(api_key=self.fallback_apis['groq'])
            completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model="llama3-8b-8192",
            )
            return completion.choices[0].message.content
        except Exception as e:
            logger.warning(f"Groq API failed: {e}")
            return self._fallback_to_local(prompt)

    def _fallback_to_local(self, prompt: str) -> str:
        """Fallback to basic local processing when APIs fail."""
        return f"iLLuMinator-4.7B: I understand you want to {prompt.lower()}. Let me help with that using local processing."

    def _find_model_path(self) -> str:
        """Find the iLLuMinator-4.7B model path."""
        possible_paths = [
            "./model/nexus_model/",
            "./model/illuminator_model/",
            "./models/iLLuMinator-4.7B/",
            os.path.expanduser("~/.cache/huggingface/transformers/"),
            "./nexus_model/"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        # Default path - will be created if needed
        return "./model/nexus_model/"
    
    def _ensure_dependencies(self):
        """Ensure required dependencies are installed."""
        required_packages = [
            "torch",
            "transformers",
            "tokenizers",
            "accelerate",
            "safetensors"
        ]
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                logger.info(f"Installing {package}...")
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    
    def _load_model_optimized(self):
        """Load the local fine-tuned model with aggressive local optimizations."""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
            import torch
            
            logger.info("ðŸš€ Loading local fine-tuned model with optimizations...")
            
            # Load tokenizer first (fastest part)
            logger.info("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                local_files_only=True,
                use_fast=False  # Use regular tokenizer to avoid tokenizer.json issues
            )
            
            # Set pad token for optimization
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Aggressive quantization for speed
            if self.use_quantization and self.device.type == "cuda":
                logger.info("Applying 8-bit quantization for GPU acceleration...")
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16,
                    bnb_8bit_use_double_quant=True,
                )
                model_kwargs = {"quantization_config": quantization_config}
            elif self.use_quantization:
                logger.info("Applying CPU optimizations...")
                model_kwargs = {"torch_dtype": torch.float16 if self.device.type != "cpu" else torch.float32}
            else:
                model_kwargs = {}
            
            # Load local model with optimizations
            logger.info("Loading optimized local fine-tuned model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                local_files_only=True,
                device_map="auto" if self.use_gpu_acceleration else None,
                low_cpu_mem_usage=True,
                **model_kwargs
            )
            
            # Set to evaluation mode for faster inference
            self.model.eval()
            
            # Enable optimization features
            if hasattr(self.model, 'gradient_checkpointing_disable'):
                self.model.gradient_checkpointing_disable()
            
            # Move to device if not using device_map
            if not self.use_gpu_acceleration or "device_map" not in model_kwargs:
                self.model = self.model.to(self.device)
            
            self.model_loaded = True
            logger.info("âœ“ iLLuMinator-4.7B optimized for local fast inference!")
            
        except ImportError as e:
            logger.error(f"Missing required packages: {e}")
            logger.info("Installing required packages...")
            self._ensure_dependencies()
            raise
        except Exception as e:
            logger.error(f"Error loading optimized iLLuMinator-4.7B model: {str(e)}")
            raise

    def _load_model(self):
        """Load the iLLuMinator-4.7B model and tokenizer."""
        try:
            # Import required libraries
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
            import torch
            
            # Configure for optimal performance
            device_map = "auto" if torch.cuda.is_available() else None
            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
            
            # Load tokenizer
            logger.info("Loading iLLuMinator-4.7B tokenizer...")
            if os.path.exists(self.model_path):
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    use_fast=True
                )
            else:
                # Load from Hugging Face Hub or GitHub
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "Anipaleja/iLLuMinator-4.7B",
                    trust_remote_code=True,
                    use_fast=True,
                    cache_dir=self.model_path
                )
            
            # Configure quantization for better memory usage
            quantization_config = None
            if torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
            
            # Load model
            logger.info("Loading iLLuMinator-4.7B model...")
            if os.path.exists(self.model_path):
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    quantization_config=quantization_config,
                    device_map=device_map,
                    torch_dtype=torch_dtype,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load from Hugging Face Hub or GitHub
                self.model = AutoModelForCausalLM.from_pretrained(
                    "Anipaleja/iLLuMinator-4.7B",
                    quantization_config=quantization_config,
                    device_map=device_map,
                    torch_dtype=torch_dtype,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    cache_dir=self.model_path
                )
            
            # Set padding token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model_loaded = True
            logger.info(f"âœ“ iLLuMinator-4.7B loaded on {self.device}")
            
        except Exception as e:
            logger.error(f"Error loading iLLuMinator-4.7B model: {str(e)}")
            # Fallback to local implementation
            self._load_local_fallback()
    
    def _load_local_fallback(self):
        """Load local fallback implementation."""
        try:
            from .illuminator_ai import IlluminatorAI
            self.model = IlluminatorAI()
            self.tokenizer = self.model.tokenizer if hasattr(self.model, 'tokenizer') else None
            self.model_loaded = True
            logger.info("âœ“ Using local iLLuMinator implementation")
        except Exception as e:
            logger.error(f"Failed to load local fallback: {str(e)}")
            self.model_loaded = False
    
    def is_available(self) -> bool:
        """Check if the model is available and loaded."""
        return self.model_loaded and self.model is not None
    
    def generate_response(self, prompt: str, max_length: int = 256, temperature: float = 0.7) -> str:
        """Generate conversational response using locally optimized fine-tuned model with quality fallback."""
        if not self.is_available():
            return "I apologize, but the local fine-tuned model is not currently available."
        
        try:
            # First try the fine-tuned model
            conversation = f"Question: {prompt}\n\nAnswer:"
            
            if hasattr(self.model, 'generate') and self.tokenizer:
                response = self._generate_with_optimized_transformers(conversation, max_length, temperature)
                
                # Check if the response is good quality
                if self._is_quality_response(response, prompt):
                    return response
                
                # If response is poor quality, use local rule-based fallback
                return self._generate_local_quality_response(prompt)
            else:
                return self._generate_local_quality_response(prompt)
            
        except Exception as e:
            logger.error(f"Response generation error: {str(e)}")
            return self._generate_local_quality_response(prompt)
    
    def _is_quality_response(self, response: str, prompt: str) -> bool:
        """Check if the generated response is of good quality with balanced criteria."""
        if not response or len(response.strip()) < 15:
            return False
        
        response_clean = response.strip()
        
        # Check for gibberish patterns and nonsensical content
        gibberish_patterns = [
            # Random characters and symbols
            r'[#%*+=\[\]{}()<>_]{2,}',
            # Excessive special characters in sequence
            r'[^\w\s.,!?-]{3,}',
            # Random code-like gibberish
            r'\s*\):\s*|\s*::\s*\*\*|\s*>>>\s*\(\s*#',
            # Nonsensical variable names
            r'[a-zA-Z]{1}[0-9]{2,}[a-zA-Z]{1}[0-9]{2,}',
            # Random brackets and operators
            r'\[\s*\|\s*=\s*\+\s*\[',
            # Underscore followed by hash/special chars at start
            r'^_\s*#',
            # Code fragments without context
            r'^\s*def\s+\w+\([^)]*\):\s*$',
            # Nonsensical questions as responses
            r'What if I am a.*but not really\?'
        ]
        
        import re
        for pattern in gibberish_patterns:
            if re.search(pattern, response_clean):
                return False
        
        # Check for meaningless responses
        meaningless_responses = [
            'code', 'hello', 'hi', '_ #', 'programming)', 'answer to a question',
            'what if i am a programmer', 'but not really'
        ]
        
        response_lower = response_clean.lower()
        if any(meaningless == response_lower.strip() for meaningless in meaningless_responses):
            return False
        
        # Check for excessive repetition
        words = response_clean.split()
        if len(words) < 6:  # Require at least 6 words for a meaningful response
            return False
        
        # Check if it's just repeated words or characters
        unique_words = set(words)
        if len(unique_words) < len(words) * 0.4:  # Less than 40% unique words
            return False
        
        # Check for meaningful sentence structure
        # Allow responses without punctuation if they're coherent and long enough
        sentences = [s.strip() for s in re.split(r'[.!?]+', response_clean) if s.strip()]
        if len(sentences) == 0:
            # If no sentences found, treat the whole response as one sentence
            sentences = [response_clean]
        
        # Check for code-like gibberish without meaning
        code_gibberish = ['def print(', 'answer 1a1c0e', 'example : i ::', 'return args """']
        if any(pattern in response_clean.lower() for pattern in code_gibberish):
            return False
        
        # For programming questions, response should contain relevant programming terms OR be informative
        if any(word in prompt.lower() for word in ['python', 'programming', 'code', 'language']):
            programming_terms = ['language', 'programming', 'code', 'software', 'development', 
                               'syntax', 'interpreter', 'compiler', 'used for', 'high-level', 'library']
            # Allow if it contains programming terms OR if it's a substantial informative response
            if not any(term in response_lower for term in programming_terms) and len(words) < 12:
                return False
        
        return True
    
    def _generate_local_quality_response(self, prompt: str) -> str:
        """Generate a quality response using comprehensive local knowledge base."""
        prompt_lower = prompt.lower()
        
        # Programming language questions
        if any(word in prompt_lower for word in ['rust', 'rust language', 'rust programming']):
            return "Rust is a systems programming language focused on safety, speed, and concurrency. It prevents segfaults and guarantees memory safety without garbage collection. Rust uses ownership, borrowing, and lifetimes to manage memory safely. It's great for system-level programming, web backends, and performance-critical applications."
        
        if any(word in prompt_lower for word in ['javascript', 'js', 'node.js']):
            return "JavaScript is a high-level programming language primarily used for web development. It's interpreted, dynamically typed, and supports both object-oriented and functional programming paradigms. JavaScript runs in browsers and on servers (Node.js), making it versatile for full-stack development."
        
        if any(word in prompt_lower for word in ['python', 'python language']):
            return "Python is a high-level, interpreted programming language known for its readability and extensive libraries. It supports multiple programming paradigms and is widely used in web development, data science, AI/ML, automation, and scientific computing. Python's philosophy emphasizes code readability and simplicity."
        
        # AI-related questions
        if any(word in prompt_lower for word in ['ai', 'artificial intelligence', 'machine learning', 'neural network', 'deep learning']):
            return "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines. It includes machine learning (algorithms that learn from data), neural networks (brain-inspired computing models), and deep learning (multi-layered neural networks). AI is used in image recognition, natural language processing, autonomous vehicles, and many other applications."
        
        # General programming questions
        if any(word in prompt_lower for word in ['programming', 'coding', 'software development', 'algorithm']):
            return "Programming is the process of creating instructions for computers using programming languages. It involves problem-solving, algorithm design, and implementing solutions in code. Key concepts include data structures, algorithms, design patterns, and software engineering principles. Popular languages include Python, JavaScript, Rust, Java, C++, and Go."
        
        # Web development
        if any(word in prompt_lower for word in ['web development', 'frontend', 'backend', 'api', 'http']):
            return "Web development involves creating websites and web applications. Frontend development focuses on user interfaces using HTML, CSS, and JavaScript. Backend development handles server logic, databases, and APIs using languages like Python, Node.js, Rust, Java, or Go. Modern web development often uses frameworks and follows RESTful or GraphQL API designs."
        
        # Data structures and algorithms
        if any(word in prompt_lower for word in ['data structure', 'algorithm', 'sorting', 'searching', 'complexity']):
            return "Data structures organize and store data efficiently (arrays, linked lists, trees, graphs, hash tables). Algorithms are step-by-step procedures to solve problems (sorting, searching, graph traversal). Time and space complexity analysis helps evaluate algorithm performance using Big O notation."
        
        # Math questions
        if any(word in prompt_lower for word in ['remainder', 'modulo', 'divide', 'math', 'calculate', 'mathematics']):
            return "In programming, mathematical operations are fundamental. The modulo operator (%) finds remainders (e.g., 9 % 8 = 1). Common math functions include addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**). Most languages provide math libraries for advanced operations like trigonometry, logarithms, and statistical functions."
        
        # Database questions
        if any(word in prompt_lower for word in ['database', 'sql', 'mongodb', 'postgresql', 'mysql']):
            return "Databases store and organize data. SQL databases (PostgreSQL, MySQL) use structured tables and relationships. NoSQL databases (MongoDB) use flexible document or key-value structures. Key concepts include CRUD operations, indexing, transactions, normalization, and query optimization."
        
        # DevOps and tools
        if any(word in prompt_lower for word in ['git', 'docker', 'kubernetes', 'devops', 'ci/cd']):
            return "DevOps practices combine development and operations. Git provides version control for code collaboration. Docker containerizes applications for consistent deployment. Kubernetes orchestrates containerized applications. CI/CD pipelines automate testing and deployment, improving software delivery speed and reliability."
        
        # Greetings
        if any(word in prompt_lower for word in ['hi', 'hello', 'hey', 'greetings']):
            return "Hello! I'm a comprehensive local AI assistant with extensive programming knowledge. I can help you with code generation in multiple languages (Python, Rust, JavaScript, Java, C++, Go, etc.), explain programming concepts, algorithms, web development, databases, and software engineering principles. What would you like to learn or build today?"
        
        # Specific "what is" questions
        if prompt_lower.startswith('what is'):
            topic = prompt_lower.replace('what is', '').strip()
            if topic in ['rust', 'javascript', 'python', 'ai', 'machine learning']:
                return self._generate_local_quality_response(topic)  # Recursively handle the topic
        
        # General fallback with more comprehensive response
        return f"I understand you're asking about '{prompt}'. I'm a comprehensive local AI assistant with knowledge in:\n\nâ€¢ Programming Languages: Python, Rust, JavaScript, Java, C++, Go, PHP, Ruby, Swift, Kotlin\nâ€¢ Web Development: Frontend/Backend, APIs, databases\nâ€¢ Algorithms & Data Structures\nâ€¢ Software Engineering & DevOps\nâ€¢ AI/ML concepts\n\nCould you be more specific about what you'd like to learn or build? I can generate code, explain concepts, or help with technical problems."
    
    def _generate_with_optimized_transformers(self, prompt: str, max_length: int = 256, temperature: float = 0.1) -> str:
        """Generate response using optimized transformers with speed optimizations."""
        try:
            import torch
            
            # Tokenize input with padding for efficiency
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512,  # Limit input length for speed
                padding=True
            )
            
            # Move to device properly
            input_ids = inputs['input_ids']
            attention_mask = inputs.get('attention_mask', None)
            
            if self.device.type != "cpu":
                input_ids = input_ids.to(self.device)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
            
            # Optimized generation settings for better quality
            generation_config = {
                "max_new_tokens": max_length,
                "do_sample": True,  # Enable sampling for variety
                "temperature": temperature,
                "top_p": 0.9,  # Nucleus sampling for quality
                "top_k": 50,   # Limit token choices for coherence
                "repetition_penalty": 1.2,  # Reduce repetitive output
                "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,  # Enable KV caching
            }
            
            # Generate with optimizations
            with torch.no_grad():  # Disable gradients for faster inference
                outputs = self.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    **generation_config
                )
            
            # Decode response
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the new generated text
            if prompt in full_response:
                response = full_response.replace(prompt, "").strip()
            else:
                # If prompt not found, try to extract after the last occurrence
                response = full_response.strip()
            
            # Additional cleanup
            response = response.strip()
            if response.startswith("Assistant:"):
                response = response[10:].strip()
            
            return response if response else "I understand your request. Let me help you with that."
            
        except Exception as e:
            logger.error(f"Optimized generation failed: {str(e)}")
            return self._generate_simple_local(prompt, max_length)

    def _generate_simple_local(self, prompt: str, max_length: int) -> str:
        """Simple local generation fallback."""
        try:
            import torch
            
            # Simple tokenization and generation
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            if self.device.type != "cpu":
                inputs = inputs.to(self.device)
            
            # Generate with minimal settings
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode and clean response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            if prompt in response:
                response = response.replace(prompt, "").strip()
            
            return response if response else "How can I help you with your coding needs?"
            
        except Exception as e:
            logger.error(f"Simple generation failed: {str(e)}")
            return "I'm here to help with your coding questions."

    def _generate_with_transformers(self, prompt: str, max_length: int = 256, temperature: float = 0.7) -> str:
        """Generate response using transformers library."""
        try:
            # Tokenize input
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", truncate=True, max_length=2048)
            inputs = inputs.to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # Decode response
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the new response part
            response = generated_text[len(prompt):].strip()
            
            # Clean up response
            response = self._clean_response(response)
            
            return response
            
        except Exception as e:
            logger.error(f"Transformers generation error: {str(e)}")
            return "I encountered an error while processing your request. Please try again."
    
    def _generate_with_local(self, prompt: str, max_length: int, temperature: float) -> str:
        """Generate response using local implementation."""
        try:
            # Use the simple local generation method
            return self._generate_simple_local(prompt, max_length)
        except Exception as e:
            logger.error(f"Local generation error: {str(e)}")
            return "I'm here to help with your coding questions. Please let me know what you need assistance with."
    
    def _clean_response(self, response: str) -> str:
        """Clean up the generated response."""
        # Remove repetitive patterns
        lines = response.split('\n')
        cleaned_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if not line or line in seen_lines:
                continue
            
            # Skip if line is too repetitive
            if (line.count('User:') > 1 or 
                line.count('Assistant:') > 1 or
                len(line.split()) > 50):
                continue
            
            cleaned_lines.append(line)
            seen_lines.add(line)
            
            # Limit response length
            if len(cleaned_lines) >= 10:
                break
        
        response = '\n'.join(cleaned_lines)
        
        # Remove any remaining artifacts
        response = response.replace('User:', '').replace('Assistant:', '').strip()
        
        if not response or len(response) < 10:
            return "I understand your question. Let me help you with that. Could you please provide more details about what you need assistance with?"
        
        return response
    
    def generate_code(self, instruction: str, language: str = "python") -> str:
        """Generate code using optimized local fine-tuned model with comprehensive language support."""
        if not self.is_available():
            return "[Error] Local fine-tuned model is not available"
        
        try:
            # Try the fine-tuned model first with better prompting
            code_prompt = f"# {language.title()} code to {instruction}\n# Code:\n"
            response = self._generate_with_optimized_transformers(code_prompt, 200, 0.7)
            code = self._extract_code_from_response(response, language)
            
            # Check if the generated code is meaningful and in the right language
            if self._is_quality_code(code, instruction) and self._is_correct_language(code, language):
                return code
            
            # Fallback to comprehensive local code generation
            return self._generate_comprehensive_local_code(instruction, language)
            
        except Exception as e:
            logger.error(f"Code generation error: {str(e)}")
            return self._generate_comprehensive_local_code(instruction, language)
    
    def _is_correct_language(self, code: str, expected_language: str) -> bool:
        """Check if the generated code is in the correct programming language."""
        expected_language = expected_language.lower()
        code_lower = code.lower()
        
        # Language-specific syntax indicators
        language_indicators = {
            'python': ['def ', 'print(', 'import ', 'if __name__', ':', 'return ', 'class '],
            'javascript': ['function', 'const ', 'let ', 'var ', '=>', 'console.log', '{', '}'],
            'java': ['public class', 'public static', 'System.out.print', 'import java', '{', '}'],
            'rust': ['fn ', 'println!', 'let ', 'mut ', 'use ', '-> ', 'impl ', 'struct '],
            'c': ['#include', 'int main', 'printf', 'return 0', '{', '}'],
            'cpp': ['#include', 'std::', 'cout', 'using namespace', '{', '}'],
            'go': ['func ', 'package ', 'import ', 'fmt.Print', '{', '}'],
            'php': ['<?php', 'echo ', '$', 'function '],
            'ruby': ['def ', 'puts ', 'class ', 'end', 'require '],
            'swift': ['func ', 'let ', 'var ', 'print(', 'import ', '{', '}'],
            'kotlin': ['fun ', 'val ', 'var ', 'println(', 'import ', '{', '}'],
        }
        
        if expected_language in language_indicators:
            indicators = language_indicators[expected_language]
            # Check if at least one language-specific indicator is present
            return any(indicator in code_lower for indicator in indicators)
        
        # If we don't have specific indicators, assume it's okay
        return True
    
    def _generate_comprehensive_local_code(self, instruction: str, language: str = "python") -> str:
        """Generate comprehensive code using local templates for any programming language."""
        instruction_lower = instruction.lower()
        language = language.lower()
        
        # Universal "hello world" / greeting patterns
        if any(word in instruction_lower for word in ['hello', 'hi', 'greet', 'say hello', 'hello world']):
            return self._generate_hello_world_code(language)
        
        # Mathematical operations
        if any(word in instruction_lower for word in ['remainder', 'modulo', 'mod', '%']):
            return self._generate_math_code(instruction, language, 'remainder')
        
        if any(word in instruction_lower for word in ['add', 'sum', 'plus', 'addition']):
            return self._generate_math_code(instruction, language, 'addition')
        
        if any(word in instruction_lower for word in ['multiply', 'product', 'times']):
            return self._generate_math_code(instruction, language, 'multiplication')
        
        # Data structures and algorithms
        if 'fibonacci' in instruction_lower:
            return self._generate_algorithm_code(language, 'fibonacci')
        
        if any(word in instruction_lower for word in ['factorial', 'fact']):
            return self._generate_algorithm_code(language, 'factorial')
        
        if any(word in instruction_lower for word in ['sort', 'bubble sort', 'quick sort']):
            return self._generate_algorithm_code(language, 'sort')
        
        # File operations
        if any(word in instruction_lower for word in ['read file', 'write file', 'file io']):
            return self._generate_file_io_code(language, instruction)
        
        # Web/HTTP related
        if any(word in instruction_lower for word in ['http', 'web server', 'api', 'request']):
            return self._generate_web_code(language, instruction)
        
        # Generic fallback with proper language syntax
        return self._generate_generic_template(instruction, language)
    
    def _generate_hello_world_code(self, language: str) -> str:
        """Generate hello world code for any programming language."""
        templates = {
            'python': '''# Python script to say hello
def say_hello():
    print("Hello, World!")
    return "Hello, World!"

# Call the function
say_hello()''',
            
            'rust': '''// Rust script to say hello
fn main() {
    println!("Hello, World!");
}

fn say_hello() -> String {
    let message = "Hello, World!";
    println!("{}", message);
    message.to_string()
}''',
            
            'javascript': '''// JavaScript script to say hello
function sayHello() {
    console.log("Hello, World!");
    return "Hello, World!";
}

// Call the function
sayHello();''',
            
            'java': '''// Java class to say hello
public class HelloWorld {
    public static void main(String[] args) {
        sayHello();
    }
    
    public static String sayHello() {
        System.out.println("Hello, World!");
        return "Hello, World!";
    }
}''',
            
            'c': '''// C program to say hello
#include <stdio.h>

void say_hello() {
    printf("Hello, World!\\n");
}

int main() {
    say_hello();
    return 0;
}''',
            
            'cpp': '''// C++ program to say hello
#include <iostream>
#include <string>

using namespace std;

string sayHello() {
    cout << "Hello, World!" << endl;
    return "Hello, World!";
}

int main() {
    sayHello();
    return 0;
}''',
            
            'go': '''// Go program to say hello
package main

import "fmt"

func sayHello() string {
    message := "Hello, World!"
    fmt.Println(message)
    return message
}

func main() {
    sayHello()
}''',
            
            'php': '''<?php
// PHP script to say hello

function sayHello() {
    $message = "Hello, World!";
    echo $message . PHP_EOL;
    return $message;
}

// Call the function
sayHello();
?>''',
            
            'ruby': '''# Ruby script to say hello
def say_hello
  message = "Hello, World!"
  puts message
  message
end

# Call the function
say_hello''',
            
            'swift': '''// Swift script to say hello
import Foundation

func sayHello() -> String {
    let message = "Hello, World!"
    print(message)
    return message
}

// Call the function
sayHello()''',
            
            'kotlin': '''// Kotlin script to say hello
fun sayHello(): String {
    val message = "Hello, World!"
    println(message)
    return message
}

fun main() {
    sayHello()
}'''
        }
        
        return templates.get(language, templates['python'])
    
    def _generate_math_code(self, instruction: str, language: str, operation: str) -> str:
        """Generate mathematical operation code for any language."""
        # Extract numbers from instruction if present
        import re
        numbers = re.findall(r'\d+', instruction)
        
        if operation == 'remainder' and len(numbers) >= 2:
            a, b = numbers[0], numbers[1]
            templates = {
                'python': f'''# Python code to find the remainder of {a} and {b}
def find_remainder(dividend, divisor):
    remainder = dividend % divisor
    return remainder

# Calculate remainder
a = {a}
b = {b}
result = find_remainder(a, b)
print(f"The remainder of {{a}} divided by {{b}} is: {{result}}")''',
                
                'rust': f'''// Rust code to find the remainder of {a} and {b}
fn find_remainder(dividend: i32, divisor: i32) -> i32 {{
    dividend % divisor
}}

fn main() {{
    let a = {a};
    let b = {b};
    let result = find_remainder(a, b);
    println!("The remainder of {{}} divided by {{}} is: {{}}", a, b, result);
}}''',
                
                'javascript': f'''// JavaScript code to find the remainder of {a} and {b}
function findRemainder(dividend, divisor) {{
    return dividend % divisor;
}}

// Calculate remainder
const a = {a};
const b = {b};
const result = findRemainder(a, b);
console.log(`The remainder of ${{a}} divided by ${{b}} is: ${{result}}`);''',
                
                'java': f'''// Java code to find the remainder of {a} and {b}
public class RemainderCalculator {{
    public static int findRemainder(int dividend, int divisor) {{
        return dividend % divisor;
    }}
    
    public static void main(String[] args) {{
        int a = {a};
        int b = {b};
        int result = findRemainder(a, b);
        System.out.println("The remainder of " + a + " divided by " + b + " is: " + result);
    }}
}}'''
            }
            return templates.get(language, templates['python'])
        
        # Generic templates for other math operations
        return self._generate_generic_math_template(instruction, language, operation)
    
    def _generate_algorithm_code(self, language: str, algorithm: str) -> str:
        """Generate algorithm implementations for any language."""
        if algorithm == 'fibonacci':
            templates = {
                'python': '''# Python Fibonacci function
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

def fibonacci_iterative(n):
    if n <= 1:
        return n
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b

# Example usage
for i in range(10):
    print(f"F({i}) = {fibonacci(i)}")''',
                
                'rust': '''// Rust Fibonacci function
fn fibonacci(n: u32) -> u32 {
    match n {
        0 => 0,
        1 => 1,
        _ => fibonacci(n - 1) + fibonacci(n - 2),
    }
}

fn fibonacci_iterative(n: u32) -> u32 {
    if n <= 1 {
        return n;
    }
    
    let mut a = 0;
    let mut b = 1;
    
    for _ in 2..=n {
        let temp = a + b;
        a = b;
        b = temp;
    }
    
    b
}

fn main() {
    for i in 0..10 {
        println!("F({}) = {}", i, fibonacci(i));
    }
}''',
                
                'javascript': '''// JavaScript Fibonacci function
function fibonacci(n) {
    if (n <= 1) {
        return n;
    }
    return fibonacci(n - 1) + fibonacci(n - 2);
}

function fibonacciIterative(n) {
    if (n <= 1) return n;
    
    let a = 0, b = 1;
    for (let i = 2; i <= n; i++) {
        [a, b] = [b, a + b];
    }
    return b;
}

// Example usage
for (let i = 0; i < 10; i++) {
    console.log(`F(${i}) = ${fibonacci(i)}`);
}'''
            }
            return templates.get(language, templates['python'])
        
        return self._generate_generic_template(f"{algorithm} algorithm", language)
    
    def _generate_generic_template(self, instruction: str, language: str) -> str:
        """Generate a generic code template for any language."""
        templates = {
            'python': f'''# Python code for: {instruction}
def main():
    # TODO: Implement {instruction}
    print("Implementing: {instruction}")
    pass

if __name__ == "__main__":
    main()''',
            
            'rust': f'''// Rust code for: {instruction}
fn main() {{
    // TODO: Implement {instruction}
    println!("Implementing: {instruction}");
}}''',
            
            'javascript': f'''// JavaScript code for: {instruction}
function main() {{
    // TODO: Implement {instruction}
    console.log("Implementing: {instruction}");
}}

main();''',
            
            'java': f'''// Java code for: {instruction}
public class Solution {{
    public static void main(String[] args) {{
        // TODO: Implement {instruction}
        System.out.println("Implementing: {instruction}");
    }}
}}''',
            
            'c': f'''// C code for: {instruction}
#include <stdio.h>

int main() {{
    // TODO: Implement {instruction}
    printf("Implementing: {instruction}\\n");
    return 0;
}}''',
            
            'go': f'''// Go code for: {instruction}
package main

import "fmt"

func main() {{
    // TODO: Implement {instruction}
    fmt.Println("Implementing: {instruction}")
}}'''
        }
        
        return templates.get(language, templates['python'])
    
    def _generate_generic_math_template(self, instruction: str, language: str, operation: str) -> str:
        """Generate generic math operation templates."""
        if language == 'python':
            return f'''# Python code for {operation}: {instruction}
def calculate_{operation}(a, b):
    # TODO: Implement {operation} operation
    result = a  # placeholder
    return result

# Example usage
num1 = 10
num2 = 5
result = calculate_{operation}(num1, num2)
print(f"Result: {{result}}")'''
        
        elif language == 'rust':
            return f'''// Rust code for {operation}: {instruction}
fn calculate_{operation}(a: i32, b: i32) -> i32 {{
    // TODO: Implement {operation} operation
    a  // placeholder
}}

fn main() {{
    let num1 = 10;
    let num2 = 5;
    let result = calculate_{operation}(num1, num2);
    println!("Result: {{}}", result);
}}'''
        
        else:
            return self._generate_generic_template(instruction, language)
    
    def _is_quality_code(self, code: str, instruction: str) -> bool:
        """Check if the generated code is of good quality."""
        if not code or len(code.strip()) < 10:
            return False
        
        # Check for minimal/bad content
        bad_indicators = ['pass', 'TODO', '# Generated code', 'hello', 'code', 'undefined', 'null', 'iterator', 'return false', 'return None', 'import']
        if any(bad in code.lower() for bad in bad_indicators):
            return False
        
        # Check for gibberish patterns
        if any(char in code for char in ['::)', '):)', '|==', ']()', '_st]', '*std']):
            return False
        
        # Check for excessive punctuation or symbols
        symbol_count = sum(1 for char in code if char in '()[]{}*&^%$#@!|\\')
        if symbol_count > len(code) * 0.3:  # More than 30% symbols
            return False
        
        # Check for basic valid syntax indicators
        if not any(keyword in code for keyword in ['def', '=', 'print', 'return', 'if', 'for', 'while', 'fn ', 'function', 'console.log', 'println!', 'printf', 'cout']):
            return False
        
        # Check if it's just repetitive nonsense
        words = code.split()
        if len(set(words)) < len(words) * 0.4:  # Less than 40% unique words
            return False
        
        return True
    
    def _generate_local_quality_code(self, instruction: str, language: str = "python") -> str:
        """Generate quality code using local templates."""
        instruction_lower = instruction.lower()
        
        if language.lower() == "python":
            # Remainder/modulo operations
            if any(word in instruction_lower for word in ['remainder', 'modulo', 'mod', '% ']):
                numbers = []
                import re
                nums = re.findall(r'\d+', instruction)
                if len(nums) >= 2:
                    return f"""# Python code to find the remainder of {nums[0]} and {nums[1]}
a = {nums[0]}
b = {nums[1]}
remainder = a % b
print(f"The remainder of {{a}} divided by {{b}} is: {{remainder}}")"""
                else:
                    return """# Python code to find remainder
def find_remainder(dividend, divisor):
    remainder = dividend % divisor
    return remainder

# Example usage
result = find_remainder(9, 8)
print(f"Remainder: {result}")"""
            
            # Hello world
            if 'hello' in instruction_lower and 'world' in instruction_lower:
                return """# Python Hello World function
def hello_world():
    print("Hello, World!")
    return "Hello, World!"

# Call the function
hello_world()"""
            
            # Fibonacci
            if 'fibonacci' in instruction_lower:
                return """# Python Fibonacci function
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# Example usage
for i in range(10):
    print(f"F({i}) = {fibonacci(i)}")"""
            
            # Addition/calculator
            if any(word in instruction_lower for word in ['add', 'sum', 'plus', 'calculator']):
                return """# Python function to add numbers
def add_numbers(a, b):
    result = a + b
    return result

# Example usage
num1 = 5
num2 = 3
total = add_numbers(num1, num2)
print(f"{num1} + {num2} = {total}")"""
        
        # Generic fallback
        return f"""# {language.title()} code for: {instruction}
# TODO: Implement the requested functionality

def main():
    # Add your code here
    pass

if __name__ == "__main__":
    main()"""
    
    def _extract_code_from_response(self, response: str, language: str) -> str:
        """Extract code from the model response, optimized for local model output."""
        # Clean the response first
        response = response.strip()
        
        # Remove the original prompt if it appears
        if f"# {language.title()} code to" in response:
            parts = response.split("# Code:\n", 1)
            if len(parts) > 1:
                response = parts[1]
        
        # Look for code blocks first
        code_patterns = [
            rf'```{language}\s*(.*?)```',
            r'```\s*(.*?)```',
        ]
        
        for pattern in code_patterns:
            import re
            matches = re.findall(pattern, response, re.DOTALL)
            if matches:
                code = matches[0].strip()
                if code and len(code) > 5:
                    return code
        
        # If no code blocks found, try to extract meaningful code lines
        lines = response.split('\n')
        code_lines = []
        
        for line in lines:
            line = line.strip()
            # Skip empty lines and obvious non-code
            if not line or line.startswith('#') and len(line.split()) < 3:
                continue
            
            # Look for code-like patterns
            if (any(keyword in line for keyword in ['def ', 'class ', 'import ', 'from ', 'if ', 'for ', 'while ', 'print(', 'return ', '=']) or
                any(symbol in line for symbol in ['{', '}', '()', '[]', ' = ']) or
                line.startswith(('    ', '\t')) or  # Indented lines
                re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*\s*=', line)):  # Variable assignments
                code_lines.append(line)
        
        if code_lines:
            # Clean up the code
            code = '\n'.join(code_lines)
            
            # Add a simple function wrapper if we just have expressions
            if language.lower() == 'python' and '=' in code and 'def ' not in code:
                return f"# Generated code\n{code}"
            
            return code
        
        # Fallback: return a simple code template
        if language.lower() == 'python':
            return f"# Python code for: {response[:50].strip()}\n# TODO: Implement the requested functionality\npass"
        else:
            return f"// Code for: {response[:50].strip()}\n// TODO: Implement the requested functionality"
    
    def analyze_code(self, code: str) -> Dict[str, Any]:
        """Analyze code structure and provide insights."""
        if not self.is_available():
            return {"error": "iLLuMinator-4.7B model is not available"}
        
        try:
            analysis_prompt = f"""Analyze this code and provide a detailed analysis in JSON format:

Code:
```
{code}
```

Provide analysis with this exact JSON structure:
{{
    "function_count": <number_of_functions>,
    "class_count": <number_of_classes>,
    "import_count": <number_of_imports>,
    "total_lines": <total_lines>,
    "code_lines": <non_empty_lines>,
    "complexity_score": <1-10_complexity_rating>,
    "functions": [
        {{"name": "function_name", "args": ["arg1", "arg2"], "line": line_number}}
    ],
    "classes": [
        {{"name": "class_name", "methods": [{{"name": "method_name", "line": line_number}}], "line": line_number}}
    ],
    "suggestions": ["improvement_suggestion_1", "improvement_suggestion_2"]
}}"""
            
            response = self.generate_response(analysis_prompt, max_length=512, temperature=0.2)
            
            # Try to extract JSON from response
            try:
                import json
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                    return analysis
            except:
                pass
            
            # Fallback analysis
            return self._fallback_code_analysis(code)
            
        except Exception as e:
            logger.error(f"Code analysis error: {str(e)}")
            return self._fallback_code_analysis(code)
    
    def _fallback_code_analysis(self, code: str) -> Dict[str, Any]:
        """Fallback code analysis using simple pattern matching."""
        import re
        
        lines = code.split('\n')
        total_lines = len(lines)
        code_lines = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
        
        functions = re.findall(r'^\s*def\s+(\w+)', code, re.MULTILINE)
        classes = re.findall(r'^\s*class\s+(\w+)', code, re.MULTILINE)
        imports = re.findall(r'^\s*(import|from)\s+', code, re.MULTILINE)
        
        return {
            "function_count": len(functions),
            "class_count": len(classes),
            "import_count": len(imports),
            "total_lines": total_lines,
            "code_lines": code_lines,
            "complexity_score": min(10, max(1, (len(functions) + len(classes)) // 2 + 1)),
            "functions": [{"name": f, "args": [], "line": 0} for f in functions],
            "classes": [{"name": c, "methods": [], "line": 0} for c in classes],
            "suggestions": ["Consider adding more comments", "Review code structure"]
        }
    
    def test_connection(self) -> bool:
        """Test if the model is working properly."""
        if not self.is_available():
            return False
        
        try:
            test_response = self.generate_response("Hello", max_length=50, temperature=0.5)
            return len(test_response.strip()) > 0
        except:
            return False
    
    def get_model_info(self) -> Dict[str, str]:
        """Get information about the loaded model."""
        return {
            "name": "iLLuMinator-4.7B",
            "version": "4.7B",
            "type": "Local Transformer Model",
            "repository": "https://github.com/Anipaleja/iLLuMinator-4.7B",
            "device": str(self.device),
            "status": "Available" if self.is_available() else "Not Available",
            "model_path": self.model_path
        }