"""
iLLuMinator API Client - Direct integration with iLLuMinator-4.7B model
Connects directly to the iLLuMinator-4.7B transformer model for local inference
Repository: https://github.com/Anipaleja/iLLuMinator-4.7B
"""

import os
import json
import time
import logging
import torch
from typing import Optional, Dict, List, Any
from pathlib import Path
import subprocess
import sys

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class iLLuMinatorAPI:
    """
    Direct API client for iLLuMinator-4.7B model from GitHub repository
    Provides local inference without external API dependencies
    """
    
    def __init__(self, model_path: Optional[str] = None, fast_mode: bool = True):
        """Initialize the iLLuMinator-4.7B model with local optimization options."""
        self.model_path = model_path or self._find_model_path()
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.conversation_history = []
        self.model_loaded = False
        self.fast_mode = fast_mode
        
        # Local performance optimizations
        self.max_length = 256 if fast_mode else 2048  # Shorter responses for speed
        self.use_quantization = fast_mode
        self.enable_caching = True
        self.use_gpu_acceleration = torch.cuda.is_available()
        
        # Optimization settings for local inference
        self.local_optimizations = {
            "use_cache": True,
            "do_sample": False,  # Faster deterministic output
            "num_beams": 1,  # No beam search for speed
            "pad_token_id": None,  # Will be set after tokenizer load
            "temperature": 0.1,  # Lower temp for faster generation
        }
        
        try:
            self._ensure_dependencies()
            self._load_model_optimized() if fast_mode else self._load_model()
            logger.info("âœ“ iLLuMinator-4.7B model loaded successfully!")
        except Exception as e:
            logger.error(f"Failed to load iLLuMinator-4.7B model: {str(e)}")
            self.model_loaded = False
    
    def _check_available_apis(self) -> Dict[str, str]:
        """Check what cloud APIs are available for fast fallback."""
        available_apis = {}
        
        # Check environment variables for API keys
        env_files = ['.env', '.env.local']
        for env_file in env_files:
            if os.path.exists(env_file):
                with open(env_file, 'r') as f:
                    for line in f:
                        if '=' in line and not line.startswith('#'):
                            key, value = line.strip().split('=', 1)
                            if 'API_KEY' in key and value:
                                if 'COHERE' in key:
                                    available_apis['cohere'] = value
                                elif 'GEMINI' in key:
                                    available_apis['gemini'] = value
                                elif 'GROQ' in key:
                                    available_apis['groq'] = value
                                elif 'OPENAI' in key:
                                    available_apis['openai'] = value
        
        # Also check from COHERE_SUCCESS.md - we know Cohere is working
        if not available_apis and os.path.exists('COHERE_SUCCESS.md'):
            with open('COHERE_SUCCESS.md', 'r') as f:
                content = f.read()
                if 'wx6ib6ezwXClSarnEZU0FrK1eLJcVTqpCAHnfuTW' in content:
                    available_apis['cohere'] = 'wx6ib6ezwXClSarnEZU0FrK1eLJcVTqpCAHnfuTW'
        
        return available_apis

    def _use_cloud_api(self, prompt: str) -> str:
        """Use cloud API for fast responses when available."""
        if 'cohere' in self.fallback_apis:
            return self._call_cohere_api(prompt)
        elif 'gemini' in self.fallback_apis:
            return self._call_gemini_api(prompt)
        elif 'groq' in self.fallback_apis:
            return self._call_groq_api(prompt)
        else:
            return "Fast mode enabled but no cloud APIs available. Using local processing..."
    
    def _call_cohere_api(self, prompt: str) -> str:
        """Call Cohere API for fast responses using requests."""
        try:
            import requests
            
            headers = {
                'Authorization': f'Bearer {self.fallback_apis["cohere"]}',
                'Content-Type': 'application/json',
            }
            
            data = {
                'model': 'command-light',
                'prompt': prompt,
                'max_tokens': 300,
                'temperature': 0.7
            }
            
            response = requests.post(
                'https://api.cohere.ai/v1/generate',
                headers=headers,
                json=data,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['generations'][0]['text'].strip()
            else:
                logger.warning(f"Cohere API error: {response.status_code}")
                return self._fallback_to_local(prompt)
                
        except Exception as e:
            logger.warning(f"Cohere API failed: {e}")
            return self._fallback_to_local(prompt)
    
    def _call_gemini_api(self, prompt: str) -> str:
        """Call Gemini API for fast responses."""
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.fallback_apis['gemini'])
            model = genai.GenerativeModel('gemini-pro')
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.warning(f"Gemini API failed: {e}")
            return self._fallback_to_local(prompt)
    
    def _call_groq_api(self, prompt: str) -> str:
        """Call Groq API for fast responses."""
        try:
            from groq import Groq
            client = Groq(api_key=self.fallback_apis['groq'])
            completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model="llama3-8b-8192",
            )
            return completion.choices[0].message.content
        except Exception as e:
            logger.warning(f"Groq API failed: {e}")
            return self._fallback_to_local(prompt)

    def _fallback_to_local(self, prompt: str) -> str:
        """Fallback to basic local processing when APIs fail."""
        return f"iLLuMinator-4.7B: I understand you want to {prompt.lower()}. Let me help with that using local processing."

    def _find_model_path(self) -> str:
        """Find the iLLuMinator-4.7B model path."""
        possible_paths = [
            "./model/nexus_model/",
            "./model/illuminator_model/",
            "./models/iLLuMinator-4.7B/",
            os.path.expanduser("~/.cache/huggingface/transformers/"),
            "./nexus_model/"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        # Default path - will be created if needed
        return "./model/nexus_model/"
    
    def _ensure_dependencies(self):
        """Ensure required dependencies are installed."""
        required_packages = [
            "torch",
            "transformers",
            "tokenizers",
            "accelerate",
            "safetensors"
        ]
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                logger.info(f"Installing {package}...")
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    
    def _load_model_optimized(self):
        """Load the iLLuMinator-4.7B model with aggressive local optimizations."""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
            import torch
            
            logger.info("ðŸš€ Loading iLLuMinator-4.7B with local optimizations...")
            
            # Load tokenizer first (fastest part)
            logger.info("Loading optimized tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                "Anipaleja/iLLuMinator-4.7B",
                cache_dir=self.model_path,
                local_files_only=False,
                trust_remote_code=True,
                use_fast=True  # Use fast tokenizer for speed
            )
            
            # Set pad token for optimization
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.local_optimizations["pad_token_id"] = self.tokenizer.pad_token_id
            
            # Aggressive quantization for speed
            if self.use_quantization and self.device.type == "cuda":
                logger.info("Applying 8-bit quantization for GPU acceleration...")
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16,
                    bnb_8bit_use_double_quant=True,
                )
                model_kwargs = {"quantization_config": quantization_config}
            elif self.use_quantization:
                logger.info("Applying CPU optimizations...")
                model_kwargs = {"torch_dtype": torch.float16}
            else:
                model_kwargs = {}
            
            # Load model with optimizations
            logger.info("Loading optimized iLLuMinator-4.7B model...")
            self.model = AutoModelForCausalLM.from_pretrained(
                "Anipaleja/iLLuMinator-4.7B",
                cache_dir=self.model_path,
                local_files_only=False,
                trust_remote_code=True,
                device_map="auto" if self.use_gpu_acceleration else None,
                low_cpu_mem_usage=True,
                **model_kwargs
            )
            
            # Set to evaluation mode for faster inference
            self.model.eval()
            
            # Enable optimization features
            if hasattr(self.model, 'gradient_checkpointing_disable'):
                self.model.gradient_checkpointing_disable()
            
            # Move to device if not using device_map
            if not self.use_gpu_acceleration or "device_map" not in model_kwargs:
                self.model = self.model.to(self.device)
            
            self.model_loaded = True
            logger.info("âœ“ iLLuMinator-4.7B optimized for local fast inference!")
            
        except ImportError as e:
            logger.error(f"Missing required packages: {e}")
            logger.info("Installing required packages...")
            self._ensure_dependencies()
            raise
        except Exception as e:
            logger.error(f"Error loading optimized iLLuMinator-4.7B model: {str(e)}")
            raise

    def _load_model(self):
        """Load the iLLuMinator-4.7B model and tokenizer."""
        try:
            # Import required libraries
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
            import torch
            
            # Configure for optimal performance
            device_map = "auto" if torch.cuda.is_available() else None
            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
            
            # Load tokenizer
            logger.info("Loading iLLuMinator-4.7B tokenizer...")
            if os.path.exists(self.model_path):
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    use_fast=True
                )
            else:
                # Load from Hugging Face Hub or GitHub
                self.tokenizer = AutoTokenizer.from_pretrained(
                    "Anipaleja/iLLuMinator-4.7B",
                    trust_remote_code=True,
                    use_fast=True,
                    cache_dir=self.model_path
                )
            
            # Configure quantization for better memory usage
            quantization_config = None
            if torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
            
            # Load model
            logger.info("Loading iLLuMinator-4.7B model...")
            if os.path.exists(self.model_path):
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    quantization_config=quantization_config,
                    device_map=device_map,
                    torch_dtype=torch_dtype,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # Load from Hugging Face Hub or GitHub
                self.model = AutoModelForCausalLM.from_pretrained(
                    "Anipaleja/iLLuMinator-4.7B",
                    quantization_config=quantization_config,
                    device_map=device_map,
                    torch_dtype=torch_dtype,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    cache_dir=self.model_path
                )
            
            # Set padding token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model_loaded = True
            logger.info(f"âœ“ iLLuMinator-4.7B loaded on {self.device}")
            
        except Exception as e:
            logger.error(f"Error loading iLLuMinator-4.7B model: {str(e)}")
            # Fallback to local implementation
            self._load_local_fallback()
    
    def _load_local_fallback(self):
        """Load local fallback implementation."""
        try:
            from .illuminator_ai import IlluminatorAI
            self.model = IlluminatorAI()
            self.tokenizer = self.model.tokenizer if hasattr(self.model, 'tokenizer') else None
            self.model_loaded = True
            logger.info("âœ“ Using local iLLuMinator implementation")
        except Exception as e:
            logger.error(f"Failed to load local fallback: {str(e)}")
            self.model_loaded = False
    
    def is_available(self) -> bool:
        """Check if the model is available and loaded."""
        return self.model_loaded and self.model is not None
    
    def generate_response(self, prompt: str, max_length: int = 256, temperature: float = 0.7) -> str:
        """Generate conversational response using iLLuMinator-4.7B model or fast cloud APIs."""
        # Fast mode: Use cloud APIs for instant responses
        if self.fast_mode and self.fallback_apis:
            logger.info("ðŸš€ Using fast cloud API for instant response")
            return self._use_cloud_api(prompt)
        
        if not self.is_available():
            return "I apologize, but the iLLuMinator-4.7B model is not currently available."
        
        try:
            # Create system context for iLLuMinator
            system_context = """You are iLLuMinator-4.7B, an advanced AI coding assistant created by Anish Paleja. You are knowledgeable, helpful, and specialize in programming and software development. You provide clear, concise answers and can help with coding problems, explanations, and technical questions.

Key capabilities:
- Code generation in multiple programming languages
- Code analysis and debugging
- Technical explanations and tutorials
- Best practices and optimization suggestions
- Project structure and architecture advice

Always provide helpful, accurate, and practical responses."""
            
            # Format the conversation
            conversation = f"{system_context}\n\nUser: {prompt}\n\nAssistant:"
            
            # Generate response with transformers
            if hasattr(self.model, 'generate') and self.tokenizer:
                return self._generate_with_transformers(conversation, max_length, temperature)
            else:
                # Use local implementation
                return self._generate_with_local(conversation, max_length, temperature)
            
        except Exception as e:
            logger.error(f"Response generation error: {str(e)}")
            # Try cloud API as fallback on error
            if self.fallback_apis:
                logger.info("Falling back to cloud API due to error")
                return self._use_cloud_api(prompt)
            return f"I apologize, but I encountered an error while generating a response: {str(e)}"
    
    def _generate_with_transformers(self, prompt: str, max_length: int, temperature: float) -> str:
        """Generate response using transformers library."""
        try:
            # Tokenize input
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", truncate=True, max_length=2048)
            inputs = inputs.to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # Decode response
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the new response part
            response = generated_text[len(prompt):].strip()
            
            # Clean up response
            response = self._clean_response(response)
            
            return response
            
        except Exception as e:
            logger.error(f"Transformers generation error: {str(e)}")
            return "I encountered an error while processing your request. Please try again."
    
    def _generate_with_local(self, prompt: str, max_length: int, temperature: float) -> str:
        """Generate response using local implementation."""
        try:
            if hasattr(self.model, 'generate_response'):
                return self.model.generate_response(prompt, max_length, temperature)
            elif hasattr(self.model, 'generate'):
                return self.model.generate(prompt, max_length, temperature)
            else:
                return "I understand your request. How can I help you with your coding needs?"
        except Exception as e:
            logger.error(f"Local generation error: {str(e)}")
            return "I'm here to help with your coding questions. Please let me know what you need assistance with."
    
    def _clean_response(self, response: str) -> str:
        """Clean up the generated response."""
        # Remove repetitive patterns
        lines = response.split('\n')
        cleaned_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if not line or line in seen_lines:
                continue
            
            # Skip if line is too repetitive
            if (line.count('User:') > 1 or 
                line.count('Assistant:') > 1 or
                len(line.split()) > 50):
                continue
            
            cleaned_lines.append(line)
            seen_lines.add(line)
            
            # Limit response length
            if len(cleaned_lines) >= 10:
                break
        
        response = '\n'.join(cleaned_lines)
        
        # Remove any remaining artifacts
        response = response.replace('User:', '').replace('Assistant:', '').strip()
        
        if not response or len(response) < 10:
            return "I understand your question. Let me help you with that. Could you please provide more details about what you need assistance with?"
        
        return response
    
    def generate_code(self, instruction: str, language: str = "python") -> str:
        """Generate code using iLLuMinator-4.7B model or fast cloud APIs."""
        # Fast mode: Use cloud APIs for instant code generation
        if self.fast_mode and self.fallback_apis:
            logger.info("ðŸš€ Using fast cloud API for instant code generation")
            code_prompt = f"Generate clean, professional {language} code for: {instruction}"
            return self._use_cloud_api(code_prompt)
        
        if not self.is_available():
            return "[Error] iLLuMinator-4.7B model is not available"
        
        try:
            # Create code generation prompt
            code_prompt = f"""You are an expert {language} programmer. Generate clean, professional, and well-commented code based on the following instruction:

Instruction: {instruction}
Language: {language}

Requirements:
- Write clean, readable code
- Include appropriate comments
- Follow best practices for {language}
- Make the code production-ready
- Include error handling where appropriate

Code:
```{language}"""
            
            # Generate code
            response = self.generate_response(code_prompt, max_length=512, temperature=0.3)
            
            # Extract code from response
            code = self._extract_code_from_response(response, language)
            
            return code
            
        except Exception as e:
            logger.error(f"Code generation error: {str(e)}")
            return f"[Error] Failed to generate code: {str(e)}"
    
    def _extract_code_from_response(self, response: str, language: str) -> str:
        """Extract code from the model response."""
        # Look for code blocks
        code_patterns = [
            rf'```{language}\s*(.*?)```',
            r'```\s*(.*?)```',
            rf'{language}\s*(.*?)(?=\n\n|$)',
        ]
        
        for pattern in code_patterns:
            import re
            matches = re.findall(pattern, response, re.DOTALL)
            if matches:
                code = matches[0].strip()
                if code and len(code) > 10:
                    return code
        
        # If no code block found, try to extract meaningful code-like content
        lines = response.split('\n')
        code_lines = []
        
        for line in lines:
            # Skip commentary lines that don't look like code
            if (any(keyword in line.lower() for keyword in ['def ', 'class ', 'import ', 'function', 'var ', 'let ', 'const ']) or
                any(symbol in line for symbol in ['{', '}', ';', '()', '[]', '=']) or
                line.strip().startswith('#') or
                line.strip().startswith('//')):
                code_lines.append(line)
        
        if code_lines:
            return '\n'.join(code_lines)
        
        # Fallback: return cleaned response
        return response.strip()
    
    def analyze_code(self, code: str) -> Dict[str, Any]:
        """Analyze code structure and provide insights."""
        if not self.is_available():
            return {"error": "iLLuMinator-4.7B model is not available"}
        
        try:
            analysis_prompt = f"""Analyze this code and provide a detailed analysis in JSON format:

Code:
```
{code}
```

Provide analysis with this exact JSON structure:
{{
    "function_count": <number_of_functions>,
    "class_count": <number_of_classes>,
    "import_count": <number_of_imports>,
    "total_lines": <total_lines>,
    "code_lines": <non_empty_lines>,
    "complexity_score": <1-10_complexity_rating>,
    "functions": [
        {{"name": "function_name", "args": ["arg1", "arg2"], "line": line_number}}
    ],
    "classes": [
        {{"name": "class_name", "methods": [{{"name": "method_name", "line": line_number}}], "line": line_number}}
    ],
    "suggestions": ["improvement_suggestion_1", "improvement_suggestion_2"]
}}"""
            
            response = self.generate_response(analysis_prompt, max_length=512, temperature=0.2)
            
            # Try to extract JSON from response
            try:
                import json
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                    return analysis
            except:
                pass
            
            # Fallback analysis
            return self._fallback_code_analysis(code)
            
        except Exception as e:
            logger.error(f"Code analysis error: {str(e)}")
            return self._fallback_code_analysis(code)
    
    def _fallback_code_analysis(self, code: str) -> Dict[str, Any]:
        """Fallback code analysis using simple pattern matching."""
        import re
        
        lines = code.split('\n')
        total_lines = len(lines)
        code_lines = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
        
        functions = re.findall(r'^\s*def\s+(\w+)', code, re.MULTILINE)
        classes = re.findall(r'^\s*class\s+(\w+)', code, re.MULTILINE)
        imports = re.findall(r'^\s*(import|from)\s+', code, re.MULTILINE)
        
        return {
            "function_count": len(functions),
            "class_count": len(classes),
            "import_count": len(imports),
            "total_lines": total_lines,
            "code_lines": code_lines,
            "complexity_score": min(10, max(1, (len(functions) + len(classes)) // 2 + 1)),
            "functions": [{"name": f, "args": [], "line": 0} for f in functions],
            "classes": [{"name": c, "methods": [], "line": 0} for c in classes],
            "suggestions": ["Consider adding more comments", "Review code structure"]
        }
    
    def test_connection(self) -> bool:
        """Test if the model is working properly."""
        if not self.is_available():
            return False
        
        try:
            test_response = self.generate_response("Hello", max_length=50, temperature=0.5)
            return len(test_response.strip()) > 0
        except:
            return False
    
    def get_model_info(self) -> Dict[str, str]:
        """Get information about the loaded model."""
        return {
            "name": "iLLuMinator-4.7B",
            "version": "4.7B",
            "type": "Local Transformer Model",
            "repository": "https://github.com/Anipaleja/iLLuMinator-4.7B",
            "device": str(self.device),
            "status": "Available" if self.is_available() else "Not Available",
            "model_path": self.model_path
        }